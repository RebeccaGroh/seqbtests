---
title: "introduction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(seqbtests)
```

This Vignette is supposed to give you a short an easy introduction to the 
seqbtest package. 
The purpose of the package is to provide a flexible and simple to use framework 
to evaluate the performance of benchmark results.

# Introduction 

This package implements different Bayesian tests to compare the performance of 
Machine Learning Algorithms. The performance of the algorithms to be compared 
can be built up within the packages by a function written by the user and can be
directly compared within the tests. For this purpose, the sequential Bayesian 
tests are provided, in which the number of generated performance values does not
have to be fixed at the beginning. Instead, after each replication it is checked 
whether there is a 95% probability to determine which algorithm performs better. 
If this threshold is reached, the build-up of replications is stopped early, 
which can result in significant time savings. These tests work either on a 
single or multiple data sets. Further, some standard Hypothesis tests are 
provided. 




# Data structure 

The data needs to be in a specific form, to be used for all tests provided in 
this package. The following table specifies the format: 

| problem | algorithm | replication | measure_\* |
|:------:|:------:|:------:|:------:|
| character | character |  integer | numeric |
| mandatory | mandatory |  mandatory | mandatory |

There is a posibility to include more than one performance measure in the data 
with just putting for example measure_acc and measure_f1 for the performance 
measure accuracy and F1-Score. It is mandatory to include at least one measure 
column along with one column defining the problem set the algorithms were tested 
on and one column containing the name of the algorithms. For the sequential 
tests further important is the replication column, with specifies the number of 
evaluation runs. 
The package has one further advantage, within the sequential tests, no data 
frame needs to be predefined. Instead the package provides the function 
get_replications(). This function calls the replications within the sequential 
tests if a data frame is provided. If no data frame is provided the user can 
define the function himself, so each run of performance evaluation is only 
called when the predefined threshold that determines which algorithm works 
better is not reached, and further evaluation needs to be done. This can save 
the user some time if not all runs have to be done for evaluation.  

To run the tests the records must not contain any NAs. If the User provides a 
complete data frame, the package provides functions to delete groups containing
NAs based on either algorithms or problem sets and gives out a complete data 
frame.


```{r}
data_with_na <- test_benchmark_small
data_with_na$measure_col[1] <- "NA" 

na_check(df = data_with_na, check_var = "algorithm") 

```

If there are NAs in the data frame, the package provides a function to drop the
group of rows containing NAs. "check_var" allows the user to decide rather to 
remove the specific problem set or algorithm. FOr this purpose the na_check 
function gives important information, as you can see whether the problem set or 
the algorithm shows a higher NA ration, i.e. which is most likely to be removed. 

```{r}
complete_data <- na_drop(df = data_with_na, check_var = "algorithm")
```

To test if the data frame already is in the right format, one can check the 
strucutre. 

```{r}
check_structure(test_benchmark_small)

```

While the tests are run, the correct format and whether all necessary columns 
are available is internally checked. 

# Frequentist Hypothesis tests 

To assess the statistical differences between the performance measure obtained by different algorithms the package provides several frequentist hypothesis tests. 

One can differentiate parametric and non-parametric tests. 

Die Tests sollten auch interpretiert werden (kurz)
Haben wir nur non-parametrische Tests ? 



Measure --> measuring the classification accuracy, precision/recall, mean 
squared eroor or further. Samplign method annd measures used for evaluating the 
performance are not that relevant for the package.  