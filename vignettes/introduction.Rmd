---
title: "introduction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(seqbtests)
```

This Vignette is supposed to give you a short an easy introduction to the 
seqbtest package. 
The purpose of the package is to provide a flexible and simple to use framework 
to evaluate the performance of benchmark results.

# Introduction 

This package implements different Bayesian tests to compare the performance of 
Machine Learning Algorithms. The performance of the algorithms to be compared 
can be built up within the packages by a function written by the user and can be
directly compared within the tests. For this purpose, the sequential Bayesian 
tests are provided, in which the number of generated performance values does not
have to be fixed at the beginning. Instead, after each replication it is checked 
whether there is a 95% probability to determine which algorithm performs better. 
If this threshold is reached, the build-up of replications is stopped early, 
which can result in significant time savings. These tests work either on a 
single or multiple data sets. Further, some standard Hypothesis tests are 
provided. 




# Data structure 

The data needs to be in a specific form, to be used for all tests provided in 
this package. The following table specifies the format: 

| problem | algorithm | replication | measure_\* |
|:------:|:------:|:------:|:------:|
| character | character |  integer | numeric |
| mandatory | mandatory |  mandatory | mandatory |

There is a posibility to include more than one performance measure in the data 
with just putting for example measure_acc and measure_f1 for the performance 
measure accuracy and F1-Score. It is mandatory to include at least one measure 
column along with one column defining the problem set the algorithms were tested 
on and one column containing the name of the algorithms. For the sequential 
tests further important is the replication column, with specifies the number of 
evaluation runs. The data frame can contain more than one measure column, 
however, for each test or plot one measure column has to be defined since it is 
not possible to compare multiple measures at once. If no column is defined, per 
default the first measure column in the data frame is used. 
The package has one further advantage, within the sequential tests, no data 
frame needs to be predefined. Instead the package provides the function 
get_replications(). This function calls the replications within the sequential 
tests if a data frame is provided. If no data frame is provided the user can 
define the function himself, so each run of performance evaluation is only 
called when the predefined threshold that determines which algorithm works 
better is not reached, and further evaluation needs to be done. This can save 
the user some time if not all runs have to be done for evaluation.  

To run the tests the records must not contain any NAs. If the User provides a 
complete data frame, the package provides functions to delete groups containing
NAs based on either algorithms or problem sets and gives out a complete data 
frame.


```{r}
data_with_na <- test_benchmark_small
data_with_na$measure_col[1] <- "NA" 

na_check(df = data_with_na, check_var = "algorithm") 

```

If there are NAs in the data frame, the package provides a function to drop the
group of rows containing NAs. "check_var" allows the user to decide rather to 
remove the specific problem set or algorithm. FOr this purpose the na_check 
function gives important information, as you can see whether the problem set or 
the algorithm shows a higher NA ration, i.e. which is most likely to be removed. 

```{r}
complete_data <- na_drop(df = data_with_na, check_var = "algorithm")
```

To test if the data frame already is in the right format, one can check the 
strucutre. 

```{r}
check_structure(test_benchmark_small)

```

While the tests are run, the correct format and whether all necessary columns 
are available is internally checked. 

# Statistical testing 

```{r}
data("test_benchmark_small")
```

The data provided in this package show examples of performance measures for 
different algorithms evaluated on a number of different data sets. They are used
to introduce the package functions. 

# Frequentist Hypothesis tests 

To assess the statistical differences between the performance measure obtained 
by different algorithms the package provides several frequentist hypothesis 
tests. 

One can differentiate parametric and non-parametric tests. The advantage of the 
non-parametric tests is that ni assumptions need to be done. If the assumptions 
for the parametric tests hold those tests a more powerful, if, however, the 
assumptions do not apply one should rather use non-parametric tests. 

Assumptions: 
* Every parametric test assumes that the data follows a certain distribution. 
    Most test assume it to be a normal distribution. 
* Data from multiple groups should show equal standard deviations. 



Before testing the differences one should always have a further look on the 
algorithms using the plot function `plot_densities` to visualize the 
distribution.

```{r}
plot_densities(test_benchmark_small)
```

The function plots the density of all algorithms performance in the considered 
data frame. As an argument the user has to define the data frame, and if the 
data contain more than one measure column the user should define which column 
to use. On the basis of the denisties one can already determine whether each 
algorithm is likely to have a normal distribution. 

To investigate the mean performance measure of the algorithms, the User can take 
a look at the boxplots. 

```{r} 
 plot_boxplot(df = test_benchmark_small)
```

The box plots show the mean performance of the algorithms across all problem 
sets in the data frame. Here you can easily see if there are strong outliers. 

The frequentist hypothesis tests test for statistiacal differences among the 
algorithms in a data frame. The null hypothesis in doing so is that there are 
no differences among the algorithms, thus all algorithm score the same 
performance. 

The package implements three different tests for statistical differnences 
namely, the correlated t-test and the non-parametric Friedman test and Wilcoxon 
Signed Ranks test. 

## Correlated t-test 

The correlated t-test is an extension of the usual t-test that takes into 
account that the performance of the algorithms is evaluated with overlapping 
trainingssets. It is used for the analysis of performance results on a single 
data set, while it accounts for correlation. 

```{r}
results_corr <- corr_t_test(df= test_benchmark_small, problemset = "problem_a", 
                       baseline = "algo_1", algorithm = "algo_2")

```

The common practice in frequentict hypothesis testing is to declare a 
significance level of p <= 0.05. If the p-value is below this threshold the 
accuracy is significantly different on that problem set. 

If no algorithm for comparing is defined, the baseline is tested against all 
algorithms in the data frame. Note that one has to take the multiple testing 
problem in account for the frequentist hypothesis tests and may correct the 
significance levlel via Bonferroni correction or other correctiom procedures. 

## Wilcoxon Signed Ranks test 

The Wilcoxon Signed Ranks test is recommended by Demsar [see @demsar2006] 
for the comaprison of two classifiers on a single data set. It is a 
non-parametric test that ranks and compares the performances of two classifiers. 

```{r}
results_wilcoxon <-  wilcoxon_signed_test(df = test_benchmark, 
                                          baseline = "algo_1", 
                                          algorithm = "algo_2", 
                                          problemset = "problem_a")  
```

The Null-Hypothesis states that the algorithms performances on the problem set 
are the same. If the p-value is smaller than 0.05 the Null-Hypothesis can be 
rejected, i.e. there are significant differences among the compared algorithms. 

## Friedman test 

The Friedman test is a non-parametric test that checks for differences among 
the algorithms performances. It has been recommended by Demsar [see @demsar2006] 
together with the corresponding post-hoc tests for comparison of classifiers 
among multiple problem sets. Therefore, it checks if there is one or more 
algorithm that perform significantly different from the others. 
It is the non-parametric equivalent to the ANOVA test. 

```{r}
results_friedman <- friedman_test(test_benchmark) 
```

The Null-Hyothesis states that all algorithms are equal. The p-value shows if 
the Null-Hypothesis can safely be rejected, i.e showing that there are 
significant differences among the algorithms. In the case of Rejection one can 
proceed wiht a post-hoc test. 




## Post-hoc tests
 
The post hoc tests are pairwise comparison of all algorithms. When there are 
significant differences among the algorithms the post-hoc tests finally shows 
among which algorithms the differences appear. 


If the Null-Hypothesis that states that all algorithms are significant is 
rejected one can proceed with a post-hoc test. This package provides one 
post-hoc test the Nemenyi post-hoc test which is recommended by Demsar 
[see @demsar2006]. 

### Nemenyi post-hoc test 

The Nemenyi test is close to the Tukey test for the ANVOA, it compares each 
algorithm to one another. If the average ranks of two algorithms differs by at 
least the value of Critical Difference (CD) the two algorithms are significantly different. 

```{r}
nemenyi_test(test_benchmark_small)
```


The performance of all algorithms can be graphically visualized via the Critical 
Differences (CD) plots, introduced by Demsar [see @demsar2006].

```{r}
plot_cd(test_benchmark_small)
```

Algorithms that are not significantly different are connected in the plot 

It is possible that the Friedman test finds significant differences the post-hoc
tests are not able to detect. This case happens quite rarely and is due to the 
post-hocs lower power. With this, one can only state that there are significant 
differences between the algorithms. 

# Bayesian Testing 

Erst einen Part, was Bayesian Testing ist. und wieso es empfohlen wird 
The Bayesian Tests provided in this package, are the tests summarized and 
recommended in Benavoli et al. [see @benavoli2017]. Its for tests, one, the
Bayesian correlated t-test for the comparison of two algorithms on one data set
and three tests, namely the Bayesian Sign test, the Bayesian Signed Rank test 
and a Bayesian hierarchical correlated t-test for testing algorithms on multiple 
data sets. 



Measure --> measuring the classification accuracy, precision/recall, mean 
squared eroor or further. Samplign method annd measures used for evaluating the 
performance are not that relevant for the package.  