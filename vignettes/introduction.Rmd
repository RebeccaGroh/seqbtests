---
title: "introduction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(seqbtests)
```

This Vignette is supposed to give you a short an easy introduction to the 
seqbtest package. 
The purpose of the package is to provide a flexible and simple to use framework 
to evaluate the performance of benchmark results.

# Introduction 

This package implements different Bayesian tests to compare the performance of 
Machine Learning Algorithms. The performance of the algorithms to be compared 
can be built up within the packages by a function written by the user and can be
directly compared within the tests. For this purpose, the sequential Bayesian 
tests are provided, in which the number of generated performance values does not
have to be fixed at the beginning. Instead, after each replication it is checked 
whether there is a 95% probability to determine which algorithm performs better. 
If this threshold is reached, the build-up of replications is stopped early, 
which can result in significant time savings. These tests work either on a 
single or multiple data sets. Further, some standard Hypothesis tests are 
provided. 




# Data structure 

The data needs to be in a specific form, to be used for all tests provided in 
this package. The following table specifies the format: 

| problem | algorithm | replication | measure_\* |
|:------:|:------:|:------:|:------:|
| character | character |  integer | numeric |
| mandatory | mandatory |  mandatory | mandatory |

There is a posibility to include more than one performance measure in the data 
with just putting for example measure_acc and measure_f1 for the performance 
measure accuracy and F1-Score. It is mandatory to include at least one measure 
column along with one column defining the problem set the algorithms were tested 
on and one column containing the name of the algorithms. For the sequential 
tests further important is the replication column, with specifies the number of 
evaluation runs. The data frame can contain more than one measure column, 
however, for each test or plot one measure column has to be defined since it is 
not possible to compare multiple measures at once. If no column is defined, per 
default the first measure column in the data frame is used. 
The package has one further advantage, within the sequential tests, no data 
frame needs to be predefined. Instead the package provides the function 
get_replications(). This function calls the replications within the sequential 
tests if a data frame is provided. If no data frame is provided the user can 
define the function himself, so each run of performance evaluation is only 
called when the predefined threshold that determines which algorithm works 
better is not reached, and further evaluation needs to be done. This can save 
the user some time if not all runs have to be done for evaluation.  

To run the tests the records must not contain any NAs. If the User provides a 
complete data frame, the package provides functions to delete groups containing
NAs based on either algorithms or problem sets and gives out a complete data 
frame.


```{r}
data_with_na <- test_benchmark_small
data_with_na$measure_col[1] <- "NA" 

na_check(df = data_with_na, check_var = "algorithm") 

```

If there are NAs in the data frame, the package provides a function to drop the
group of rows containing NAs. "check_var" allows the user to decide rather to 
remove the specific problem set or algorithm. FOr this purpose the na_check 
function gives important information, as you can see whether the problem set or 
the algorithm shows a higher NA ration, i.e. which is most likely to be removed. 

```{r}
complete_data <- na_drop(df = data_with_na, check_var = "algorithm")
```

To test if the data frame already is in the right format, one can check the 
strucutre. 

```{r}
check_structure(test_benchmark_small)

```

While the tests are run, the correct format and whether all necessary columns 
are available is internally checked. 

# Statistical testing 

```{r}
data("test_benchmark_small")
```

The data provided in this package show examples of performance measures for 
different algorithms evaluated on a number of different data sets. They are used
to introduce the package functions. 

## Frequentist Hypothesis tests 

To assess the statistical differences between the performance measure obtained 
by different algorithms the package provides several frequentist hypothesis 
tests. 

One can differentiate parametric and non-parametric tests. If the assumptions 
for the parametrc tests hold the tests a more powerful, if, however if the 
assumptions do not apply one should rather use non-parametric tests. 

Assumptions: 
* Every parametric test assumes that the data follows a certain distribution. 
    Most test assume it to be a normal distribution. 
* Data from multiple groups should show equal standard deviations. 

Before testing the differences one should always have a furthe look on the 
algorithms using the plot function `plot_densities` to visualize the 
distribution.

```{r}
plot_densities(test_benchmark_small)
```

The function plots the density of all algorithms performance in the considered 
data frame. As an argument the user has to define the data frame, and if the 
data contain more than one measure column the user should define which column 
to use. On the basis of the denisties one can already determine whether each 
algorithm is likely to have a normal distribution. 

To investigate the mean performance measure of the algorithms, the User can take 
a look at the boxplots. 

```{r} 
 plot_boxplot(df = test_benchmark_small)
```

The box plots show the mean performance of the algorithms across all problem 
sets in the data frame. Here you can easily see if there are strong outliers. 

The frequentist hypothesis tests test for statistiacal differences among the 
algorithms in a data frame. The null hypothesis in doing so is that there are 
no differences among the algorithms, thus all algorithm score the same 
performance. 

The package implements three different tests for statistical differnences 
namely, the correlated t-test and the non-parametric Friedman test and Wilcoxon 
Signed Ranks test. 



Die Tests sollten auch interpretiert werden (kurz)
Haben wir nur non-parametrische Tests ? 



Measure --> measuring the classification accuracy, precision/recall, mean 
squared eroor or further. Samplign method annd measures used for evaluating the 
performance are not that relevant for the package.  