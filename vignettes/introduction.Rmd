---
title: "Sequential Bayesian Comparison of ML Algorithm Performances"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r load.libraries, message=FALSE, warning=FALSE}
library(ggplot2)
library(seqbtests)
```

This Vignette is supposed to give you a short an easy introduction to the 
seqbtest package. 
The purpose of the package is to provide a flexible and simple to use framework 
to evaluate the performance of benchmark results.

# Introduction 

This package implements different Bayesian tests to compare the performance of 
Machine Learning Algorithms. The performance of the algorithms to be compared 
can be built up within the packages by a function written by the user and can be
directly compared within the tests. For this purpose, the sequential Bayesian 
tests are provided, in which the number of generated performance values does not
have to be fixed at the beginning. Instead, after each replication it is checked 
whether there is a 95% probability to determine which algorithm performs better. 
If this threshold is reached, the build-up of replications is stopped early, 
which can result in significant time savings. These tests work either on a 
single or multiple data sets. Further, some standard Hypothesis tests are 
provided. 


# Data structure 

The data needs to be in a specific form, to be used for all tests provided in 
this package. The following table specifies the format: 

| problem | algorithm | replication | measure_\* |
|:------:|:------:|:------:|:------:|
| character | character |  integer | numeric |
| mandatory | mandatory |  mandatory | mandatory |

There is a posibility to include more than one performance measure in the data 
with just putting for example measure_acc and measure_mse for the performance 
measure accuracy and F1-Score. It is mandatory to include at least one measure 
column along with one column defining the problem set the algorithms were tested 
on and one column containing the name of the algorithms. For the sequential 
tests further important is the replication column, with specifies the number of 
evaluation runs. The data frame can contain more than one measure column, 
however, for each test or plot one measure column has to be defined since it is 
not possible to compare multiple measures at once. If no column is defined, per 
default the first measure column in the data frame is used. 
FOr the sake of simplicity, the data frames containingf the evaluation data are
referred to as data frames in the following, while the data sets used to 
evaluate the classifiers performance are referred to as problem sets. 
The package has one further advantage, within the sequential tests, no data 
frame needs to be predefined. Instead the package provides the function 
get_replications(). This function calls the replications within the sequential 
tests if a data frame is provided. If no data frame is provided the user can 
define the function himself, so each run of performance evaluation is only 
called when the predefined threshold that determines which algorithm works 
better is not reached, and further evaluation needs to be done. This can save 
the user some time if not all runs have to be done for evaluation.  

To run the tests the records must not contain any NAs. If the User provides a 
complete data frame, the package provides functions to delete groups containing
NAs based on either algorithms or problem sets and gives out a complete data 
frame.


```{r}
data_with_na <- test_benchmark_small
data_with_na$measure_col[1] <- "NA" 

na_check(df = data_with_na, check_var = "algorithm") 

```

If there are NAs in the data frame, the package provides a function to drop the
group of rows containing NAs. "check_var" allows the user to decide rather to 
remove the specific problem set or algorithm. FOr this purpose the na_check 
function gives important information, as you can see whether the problem set or 
the algorithm shows a higher NA ration, i.e. which is most likely to be removed. 

```{r}
complete_data <- na_drop(df = data_with_na, check_var = "algorithm")
```

To test if the data frame already is in the right format, one can check the 
strucutre. 

```{r}
check_structure(test_benchmark_small)

```

While the tests are run, the correct format and whether all necessary columns 
are available is internally checked. 

# Statistical testing 

```{r}
data("test_benchmark_small")
```

The data provided in this package show examples of performance measures for 
different algorithms evaluated on a number of different data sets. They are used
to introduce the package functions. 

# Frequentist Hypothesis tests 

To assess the statistical differences between the performance measure obtained 
by different algorithms the package provides several frequentist hypothesis 
tests. 

One can differentiate parametric and non-parametric tests. The advantage of the 
non-parametric tests is that ni assumptions need to be done. If the assumptions 
for the parametric tests hold those tests a more powerful, if, however, the 
assumptions do not apply one should rather use non-parametric tests. 

Assumptions: 
* Every parametric test assumes that the data follows a certain distribution. 
    Most test assume it to be a normal distribution. 
* Data from multiple groups should show equal standard deviations. 



Before testing the differences one should always have a further look on the 
algorithms using the plot function `plot_densities` to visualize the 
distribution.

```{r}
plot_densities(test_benchmark_small)
```

The function plots the density of all algorithms performance in the considered 
data frame. As an argument the user has to define the data frame, and if the 
data contain more than one measure column the user should define which column 
to use. On the basis of the denisties one can already determine whether each 
algorithm is likely to have a normal distribution. 

To investigate the mean performance measure of the algorithms, the User can take 
a look at the boxplots. 

```{r} 
 plot_boxplot(df = test_benchmark_small)
```

The box plots show the mean performance of the algorithms across all problem 
sets in the data frame. Here you can easily see if there are strong outliers. 

The frequentist hypothesis tests test for statistiacal differences among the 
algorithms in a data frame. The null hypothesis in doing so is that there are 
no differences among the algorithms, thus all algorithm score the same 
performance. 

The package implements three different tests for statistical differnences 
namely, the correlated t-test and the non-parametric Friedman test and Wilcoxon 
Signed Ranks test. 

## Correlated t-test 

The correlated t-test is an extension of the usual t-test that takes into 
account that the performance of the algorithms is evaluated with overlapping 
trainingssets. It is used for the analysis of performance results on a single 
data set, while it accounts for correlation. 

```{r}
results_corr <- corr_t_test(df= test_benchmark_small, problem = "problem_a", 
                       baseline = "algo_1", algorithm = "algo_2")

```

The common practice in frequentict hypothesis testing is to declare a 
significance level of p <= 0.05. If the p-value is below this threshold the 
accuracy is significantly different on that problem set. 

If no algorithm for comparing is defined, the baseline is tested against all 
algorithms in the data frame. Note that one has to take the multiple testing 
problem in account for the frequentist hypothesis tests and may correct the 
significance levlel via Bonferroni correction or other correctiom procedures. 

## Wilcoxon Signed Ranks test 

The Wilcoxon Signed Ranks test is recommended by Demsar [see @demsar2006] 
for the comaprison of two classifiers on a single data set. It is a 
non-parametric test that ranks and compares the performances of two classifiers. 

```{r}
results_wilcoxon <-  wilcoxon_signed_test(df = test_benchmark, 
                                          baseline = "algo_1", 
                                          algorithm = "algo_2", 
                                          problem = "problem_a")  
```

The Null-Hypothesis states that the algorithms performances on the problem set 
are the same. If the p-value is smaller than 0.05 the Null-Hypothesis can be 
rejected, i.e. there are significant differences among the compared algorithms. 

## Friedman test 

The Friedman test is a non-parametric test that checks for differences among 
the algorithms performances. It has been recommended by Demsar [see @demsar2006] 
together with the corresponding post-hoc tests for comparison of classifiers 
among multiple problem sets. Therefore, it checks if there is one or more 
algorithm that perform significantly different from the others. 
It is the non-parametric equivalent to the ANOVA test. 

```{r}
results_friedman <- friedman_test(test_benchmark) 
```

The Null-Hyothesis states that all algorithms are equal. The p-value shows if 
the Null-Hypothesis can safely be rejected, i.e showing that there are 
significant differences among the algorithms. In the case of Rejection one can 
proceed wiht a post-hoc test. 




## Post-hoc tests
 
The post hoc tests are pairwise comparison of all algorithms. When there are 
significant differences among the algorithms the post-hoc tests finally shows 
among which algorithms the differences appear. 


If the Null-Hypothesis that states that all algorithms are significant is 
rejected one can proceed with a post-hoc test. This package provides one 
post-hoc test the Nemenyi post-hoc test which is recommended by Demsar 
[see @demsar2006]. 

### Nemenyi post-hoc test 

The Nemenyi test is close to the Tukey test for the ANVOA, it compares each 
algorithm to one another. If the average ranks of two algorithms differs by at 
least the value of Critical Difference (CD) the two algorithms are significantly 
different. 

```{r}
nemenyi_test(test_benchmark_small)
```


The performance of all algorithms can be graphically visualized via the Critical 
Differences (CD) plots, introduced by Demsar [see @demsar2006].

```{r}
plot_cd(test_benchmark_small)
```

Algorithms that are not significantly different are connected in the plot 

It is possible that the Friedman test finds significant differences the post-hoc
tests are not able to detect. This case happens quite rarely and is due to the 
post-hocs lower power. With this, one can only state that there are significant 
differences between the algorithms. 

# Bayesian Testing 

In the analysis of the performance of alforithms, frequentist tests have been 
used for may years, while in other scientific fields it has been establisghed 
for a while now that Bayesian approaches have many advantages instead. Benavoli 
et al. [@benavoli2017] propose the use of Bayesian tests for comparing the 
performance of machine learning algorithms. The Bayesian tests model the 
posterior probability of the parameters and can be used to the estimate the 
probability of three different outcomes. While the null hypothesis of the 
frequentist tests is that there are no differences between the performances of 
the algorithms, the Bayesina approaches provide more information. On the one 
hand, the results are calculated for hwether the baseline is better than the 
compare algorithm (P(Baseline >> Algorithm)), the probability that the algorithm 
perfroms better than the Baselien (P(Baseline << Algorithm)) and additional the 
probability that the two algorithms are practically equivalent 
(P(Baseline = Algorithm)). While the nulll hypothesis can only be rejected in 
the frequentist tests, Bayesian tests allow to make a statement if the posterior 
proability of the two classifiers is practically the same. This can be achieved 
by determining teh ROPE (Region Of Practical Equivalende). Usually two 
algorithms are practically equivalent if the mean differences of accuracies are 
less than 1%. 

The advantages of Baysesian tests are therefore: 
* Estimation of the posterior probability for a meaningful Null Hypothesis 
* If the algorithms are not practically the same, one can still make sensible 
    statements about the behaviour of the algorithms
* The posterior probabilities display both the effects magnitude and the 
    uncertainty 
* The posterior proabilities don't depend on the sampling intentions.

The Bayesian Tests provided in this package, are the tests summarized and 
recommended in Benavoli et al. [see @benavoli2017]. Its for tests, one, the
Bayesian correlated t-test for the comparison of two algorithms on one data set
and three tests, namely the Bayesian Sign test, the Bayesian Signed Rank test 
and a Bayesian hierarchical correlated t-test for testing algorithms on multiple 
data sets. 

## Bayesian correlated t-test 

The Bayesian correlated t-test compares the performance of two competign 
classifiers on a single problem set. It is the Bayesian counterpart for the 
frequentist correlated t-test and thus also takes correlation due to overlapping 
trainings data into account. As the usual correlated t-test, the Bayesian 
counterpart is a parametric test, which assumes Gaussian distribution of the 
data. 
For rho is 0 the Bayesian correlated t-test and the frequentist correlated 
t-test are numerically equivalent. Yet, the approaches inferences are different. 
The different interpretation of the same numerical values is different and thus, 
changes the perspectives. 

```{r}
results_b_corr <- b_corr_t_test(df= test_benchmark_small, 
                                problem = "problem_a", 
                                baseline = "algo_1", algorithm = "algo_5")
```

The correlation "rho" has to be between 0 and 1, the variance needs to be > 0. 
The "better" column shows either TRUE, if the Baseline performs better than the 
compared algorithm for the given threshold, or else FALSE. 
The function returns a dataframe with the results of each comparison in one row. 
The User can either define an algorithm, that should be tested against the 
Baseline, if no algorithm is defined, the Baseline will be tested against all 
Algorithms in the data frame. 
The columns for left, rope and rigth show the posterior probabilities. The 
column labeled "left defines the posterior probability of the differences 
between the baseline and the compared algorithm being below the bound of the 
rope (P(Baseline >> Algorithm)). The column "rope" determines the posterior 
probability being inside the rope (P(Baseline = Algorithm)). Thus, the "right" 
column shows the probability of being above the upper bound 
(P(Baseline << Algorithm)). The column "better" indicates if the Baseline 
performs better with a probability defined by the User (per default 95% is 
assumed).  

```{r}
plot_posterior(results_b_corr, method = "b_corr_t_test")
```

The posterior probability can be plotted with the function "plot_posterior". It
is a graphically visualization of the results. For the plot function two 
arguments need to be defined. First the results that should be visualized. 
Although the test allow to compare multiple algorithms against the Baseline, for 
the plot function, the algorithm needs to be defined. The second argument is the 
test that is performed via "method".
The plot visualtizes the posterior probabilities, as well as the ROPE. 
P(Baseline = Algorithm is the integral of the posterior distribution inbetween 
the ROPE, which is defined by the twoe vertical lines. The width of the plots 
corresponds to the uncertainty in the data. 
If all the probability mass is inside the ROPE one can conclude that the 
Baseline and ALgorithm are practically equivalent.  


## Bayesian Sign test and Bayesian Signed Ranks test

The Bayesian Sign and Signed Ranks tests compare classifiers on multiple problem 
sets. They are based on the non-parametric frequentist counterparts the Sign and
Wilcoxon Signed Ranks test and assume a Dirichlet Process (DP) prior. 
Different than the correlated t-test these tests don't take the correlation into 
account. But because of the non-parametric nature of the tests, they do not 
assume normality of the samples mean, is robust regarding outliers and does not 
assume commensurability.

The Bayesian Sign test is less powerful than the Bayesian Signed Rank test. 
Since the Signed Ranks test does not only take the sign into account, but also 
the size of differences. It ranks the absolute values of differences and 
compares these ranks, while considering the signs. 

```{r}
results_b_sign <- b_sign_test(df= test_benchmark_small, 
                              problem = "problem_a", 
                              baseline = "algo_1", algorithm = "algo_2")
results_b_signed <- b_signed_rank_test(df= test_benchmark_small, 
                                       baseline = "algo_1", 
                                       algorithm = "algo_2")
```

The graphical visualization can be called just the same way as for the 
correlated t test via "plot_posterior" the User only needs to define the new 
method used. 
In this plot the posterior distributions of the probabilities are plotted for 
"left", "rope" and "right" are displayed in a triangle plot. 

```{r}
plot_posterior(results_b_sign, method = "b_sign_test")
plot_posterior(results_b_signed, method = "b_signed_rank_test")
```


## Bayesian Hierarchical correlated t-test 

The Bayesian hierarchical correlated t-test is an extension of the correlated 
t-test that works for the compariosn of classifiers on multiple problem sets and 
still takes correlation into account 
```{r}
results_b_hierarchical <- b_hierarchical_test(df= test_benchmark_small, 
                                            baseline = "algo_1", 
                                            algorithm = "algo_2",  
                                            rho=0.1, rope=c(-0.01, 0.01), 
                                            nsim=2000,  nchains=5)
```

The Hierarchical approach is recommende by Benavoli [@benavoli2017] over the 
Bayesian Signed test. Yet, it is quite time consuming, which is inconvenient if 
the tests needs to be run often. 

The graphical visualization shows a similar triangle plot as the Bayesian Sign 
and Signed Ranks test. 

```{r}
plot_posterior(results_b_hierarchical, method = "b_hierarchical_test")
```



# Sequential Bayesian Testing 

While all the former tests have been implemented in different forms in other 
packages, this packge is trying to otimize the process of evaluation of 
different algorithms. 
The procedure of assessing the algorithms performance is mostly quite time-
consuming. To counteract this issue this package provides a sequential approach 
for evaluating performance measure using Bayesian tests. Thereby the results of 
the comparisons are checked after each run of evaluation of the performances. If 
the baseline algorithm has high probability (the user can define the parameter 
himself, by default 95% is used) to have a better performance than the compared 
algorithm, all further testing is aborted after this run and the early results 
are used. If all algorithms in the data frame are tested against the baseline 
and the baseline outperforms on algorithms after a few runs, the analysis is 
terminated for this specific algorithm, yet the procedure continues for the 
other algorithms. 
In this package the user has the possibility to generate each new run of 
performance evaluation within the Bayesian tests. This works via the function 
“get_replications” which can be used to build a data frame, where after each run
a Bayesian test is performed to check if the threshold has already been reached,
i.e. that the Baseline performs better than the other algorithm with a 
probability of 95%. If the User is working with an already complete data frame, 
he does not have to use an own  function but can call the default function for 
“get_replications”, where the complete data frame is called by the argument 
“df =”. The “max_repls” can be used to specify the maximum number of runs that 
should be generated. If a complete record is provided “max_repls” should 
correspond to the number of runs in the record. 
When customizing the test, the User also has the option to specify whether the 
Baseline algorithm should be tested to be better or at least as good as the 
algorithm being tested against. This can be defined via the argument "compare" 
by specifying either "better" or "equal". This is especially important when it 
comes to comparing two almost similar algorithms, which differ in their 
computational effort. Here it can often be helpful to know that the 
computationally less complex algorithm performs at least as well as the other 
one.
With Bayesian tests, multiple testing problems usually do not play a role. You 
can either avoid this by multilevel analysis, like the hierarchical model. You 
can also reduce the problem by using the rope. 
Extensive statistical experiments have shown that there is no early stopping 
bias due to the sequential approach if a minimum number of replications is drawn
before the stopping action is carried out. This minimum number is set to 5 in 
this package after simulation studies have shown that this number is sufficient 
to achieve valid results. However, the user can also set a minimum number 
himself via "number_runs".


```{r}
results_seq_corr <- seq_b_corr_t_test(df = test_benchmark_small, rho=0.1,
                             problem = "problem_a", baseline = "algo_1", 
                             compare = "equal", max_repls = 10)
results_seq_sign <- seq_b_sign_test(df = test_benchmark_small, 
                                    baseline = "algo_1", max_repls = 10)
results_seq_ranks <- seq_b_signed_rank_test(df = test_benchmark_small,
                                            baseline = 'algo_1', max_repls = 10)
results_seq_hierarchical <- seq_b_hierarchical_test(df = test_benchmark_small,
                                        baseline = 'algo_1', max_repls = 10)


```

For further information regarding the sequential bayesian tests see Groh 
[@groh2019]. 


 kann man vielleicht noch irgendwo einfügen 
Measure --> measuring the classification accuracy, precision/recall, mean 
squared eroor or further. Samplign method annd measures used for evaluating the 
performance are not that relevant for the package.  